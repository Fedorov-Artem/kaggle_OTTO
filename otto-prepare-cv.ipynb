{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Prepare cross-validation\nThis notebook prepares the cross-validation datasets for the OTTO competition. It mostly uses code provided by organizers. They've used the code to produce test dataset. I use the same code to produce two cross-validation datasets.\nAlso, the notebook transform all the datasets, both test and cross-validation from json to dataframe and saves these dataset as parquet files to be used by all other notebooks of the project.\n\n## Imports and definitions","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport gc\nimport argparse\nimport json\nimport random\nfrom copy import deepcopy\nfrom pathlib import Path\n\nfrom pandas.io.json._json import JsonReader\nfrom tqdm.auto import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-12-16T13:26:14.026152Z","iopub.execute_input":"2022-12-16T13:26:14.026905Z","iopub.status.idle":"2022-12-16T13:26:14.174702Z","shell.execute_reply.started":"2022-12-16T13:26:14.026794Z","shell.execute_reply":"2022-12-16T13:26:14.173814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install beartype\nfrom beartype import beartype","metadata":{"execution":{"iopub.status.busy":"2022-12-16T13:26:19.451104Z","iopub.execute_input":"2022-12-16T13:26:19.451526Z","iopub.status.idle":"2022-12-16T13:26:35.152353Z","shell.execute_reply.started":"2022-12-16T13:26:19.451488Z","shell.execute_reply":"2022-12-16T13:26:35.151093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Code provided by competition organizers. \n# They used it to prepare test dataset, and I use it to prepare the cross-validation dataset.\nclass setEncoder(json.JSONEncoder):\n\n    def default(self, obj):\n        return list(obj)\n\n    \n@beartype\n#def ground_truth(events: list[dict]):\ndef ground_truth(events: list):\n    prev_labels = {\"clicks\": None, \"carts\": set(), \"orders\": set()}\n\n    for event in reversed(events):\n        event[\"labels\"] = {}\n\n        for label in ['clicks', 'carts', 'orders']:\n            if prev_labels[label]:\n                if label != 'clicks':\n                    event[\"labels\"][label] = prev_labels[label].copy()\n                else:\n                    event[\"labels\"][label] = prev_labels[label]\n\n        if event[\"type\"] == \"clicks\":\n            prev_labels['clicks'] = event[\"aid\"]\n        if event[\"type\"] == \"carts\":\n            prev_labels['carts'].add(event[\"aid\"])\n        elif event[\"type\"] == \"orders\":\n            prev_labels['orders'].add(event[\"aid\"])\n\n    return events[:-1]\n\n@beartype\n#def split_events(events: list[dict], split_idx=None):\ndef split_events(events: list, split_idx=None):\n    test_events = ground_truth(deepcopy(events))\n    if not split_idx:\n        split_idx = random.randint(1, len(test_events))\n    test_events = test_events[:split_idx]\n    labels = test_events[-1]['labels']\n    for event in test_events:\n        del event['labels']\n    return test_events, labels\n\n\n@beartype\ndef create_kaggle_testset(sessions: pd.DataFrame, sessions_output: Path, labels_output: Path):\n    last_labels = []\n    splitted_sessions = []\n\n    for _, session in tqdm(sessions.iterrows(), desc=\"Creating trimmed testset\", total=len(sessions)):\n        session = session.to_dict()\n        splitted_events, labels = split_events(session['events'])\n        last_labels.append({'session': session['session'], 'labels': labels})\n        splitted_sessions.append({'session': session['session'], 'events': splitted_events})\n\n    with open(sessions_output, 'w') as f:\n        for session in splitted_sessions:\n            f.write(json.dumps(session) + '\\n')\n\n    with open(labels_output, 'w') as f:\n        for label in last_labels:\n            f.write(json.dumps(label, cls=setEncoder) + '\\n')\n\n\n@beartype\ndef trim_session(session: dict, max_ts: int) -> dict:\n    session['events'] = [event for event in session['events'] if event['ts'] < max_ts]\n    return session\n\n\n@beartype\ndef get_max_ts(sessions_path: Path) -> int:\n    max_ts = float('-inf')\n    with open(sessions_path) as f:\n        for line in tqdm(f, desc=\"Finding max timestamp\"):\n            session = json.loads(line)\n            max_ts = max(max_ts, session['events'][-1]['ts'])\n    return max_ts\n\n\n@beartype\n#def filter_unknown_items(session_path: Path, known_items: set[int]):\ndef filter_unknown_items(session_path: Path, known_items: set):\n    filtered_sessions = []\n    with open(session_path) as f:\n        for line in tqdm(f, desc=\"Filtering unknown items\"):\n            session = json.loads(line)\n            session['events'] = [event for event in session['events'] if event['aid'] in known_items]\n            if len(session['events']) >= 2:\n                filtered_sessions.append(session)\n    with open(session_path, 'w') as f:\n        for session in filtered_sessions:\n            f.write(json.dumps(session) + '\\n')\n\n\n@beartype\ndef train_test_split(session_chunks: JsonReader, train_path: Path, test_path: Path, max_ts: int, test_days: int):\n    split_millis = test_days * 24 * 60 * 60 * 1000\n    split_ts = max_ts - split_millis\n    train_items = set()\n    Path(train_path).parent.mkdir(parents=True, exist_ok=True)\n    train_file = open(train_path, \"w\")\n    Path(test_path).parent.mkdir(parents=True, exist_ok=True)\n    test_file = open(test_path, \"w\")\n    for chunk in tqdm(session_chunks, desc=\"Splitting sessions\"):\n        for _, session in chunk.iterrows():\n            session = session.to_dict()\n            if session['events'][0]['ts'] > split_ts:\n                test_file.write(json.dumps(session) + \"\\n\")\n            else:\n                session = trim_session(session, split_ts)\n                if len(session['events']) >= 2:\n                    train_items.update([event['aid'] for event in session['events']])\n                    train_file.write(json.dumps(session) + \"\\n\")\n    train_file.close()\n    test_file.close()\n    filter_unknown_items(test_path, train_items)\n\n\n@beartype\ndef main(train_set: Path, output_path: Path, days: int, seed: int):\n    random.seed(seed)\n    max_ts = get_max_ts(train_set)\n\n    session_chunks = pd.read_json(train_set, lines=True, chunksize=100000)\n    train_file = output_path / 'train_sessions.jsonl'\n    test_file_full = output_path / 'test_sessions_full.jsonl'\n    train_test_split(session_chunks, train_file, test_file_full, max_ts, days)\n\n    test_sessions = pd.read_json(test_file_full, lines=True)\n    test_sessions_file = output_path / 'test_sessions.jsonl'\n    test_labels_file = output_path / 'test_labels.jsonl'\n    create_kaggle_testset(test_sessions, test_sessions_file, test_labels_file)\n\n'''\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--train-set', type=Path, required=True)\n    parser.add_argument('--output-path', type=Path, required=True)\n    parser.add_argument('--days', type=int, default=2)\n    parser.add_argument('--seed', type=int, default=42)\n    args = parser.parse_args()\n    main(args.train_set, args.output_path, args.days, args.seed)\n'''","metadata":{"execution":{"iopub.status.busy":"2022-12-16T13:26:52.749279Z","iopub.execute_input":"2022-12-16T13:26:52.749692Z","iopub.status.idle":"2022-12-16T13:26:52.799466Z","shell.execute_reply.started":"2022-12-16T13:26:52.749657Z","shell.execute_reply":"2022-12-16T13:26:52.797977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Transfrorm cross-validation labels from json to dataframe.\ndef save_labels(data_path):\n    chunks = pd.read_json(data_path, lines=True, chunksize=100_000)\n    labels_df =  pd.DataFrame()\n    \n    for chunk in tqdm(chunks):\n        #df_labels = pd.DataFrame(chunk)\n        label_dict = {\n            'session': [],\n            'clicks': [],\n            'carts': [],\n            'orders': [],\n            }\n        for session, labels in zip(chunk['session'].tolist(), chunk['labels'].tolist()):\n            label_dict['session'].append(session)\n            if 'clicks' in labels:\n                label_dict['clicks'].append(labels['clicks'])\n            else:\n                label_dict['clicks'].append(-1)\n            if 'carts' in labels:\n                label_dict['carts'].append(labels['carts'])\n            else:\n                label_dict['carts'].append([])\n            if 'orders' in labels:\n                label_dict['orders'].append(labels['orders'])\n            else:\n                label_dict['orders'].append([])\n        chunk_labels = pd.DataFrame(label_dict)\n        chunk_labels['session'] = chunk_labels['session'].astype(np.int32)\n        chunk_labels['clicks'] = chunk_labels['clicks'].astype(np.int32)\n        labels_df = pd.concat([labels_df, chunk_labels])\n    labels_df = labels_df.reset_index(drop=True)\n    return labels_df\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Transfrorm all other json files to dataframes and reduce memory usage.\n\ndef load_transform_json(sessions_df, file_path):\n    chunks = pd.read_json(file_path, lines=True, chunksize=100_000)\n    for e, chunk in enumerate(chunks):\n        print(e)\n        event_dict = {\n            'session': [],\n            'aid': [],\n            'ts': [],\n            'type': [],\n        }\n        \n        for session, events in zip(chunk['session'].tolist(), chunk['events'].tolist()):\n            for event in events:\n                event_dict['session'].append(session)\n                event_dict['aid'].append(event['aid'])\n                event_dict['ts'].append(event['ts'])\n                event_dict['type'].append(event['type'])\n        chunk_session = pd.DataFrame(event_dict)\n        chunk_session['session'] = chunk_session['session'].astype(np.int32)\n        chunk_session['aid'] = chunk_session['aid'].astype(np.int32)\n        chunk_session['ts'] = chunk_session['ts']/1000\n        chunk_session['ts'] = chunk_session['ts'].astype(np.int32)\n        chunk_session['type'] = chunk_session['type'].map({'clicks': 0, 'carts': 1,  'orders': 2}).astype(np.int8)\n        sessions_df = pd.concat([sessions_df, chunk_session])\n        del chunk_session, event_dict\n        gc.collect()\n    sessions_df = sessions_df.reset_index(drop=True)\n    return sessions_df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# A simple procedure to load a json file, convert it to tabular data and save as a parquet file.\ndef load_save(input_path, output_name):\n    df_load_save =  pd.DataFrame()\n    df_load_save = load_transform_json(df_load_save, input_path)\n    df_load_save.to_parquet(output_name)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create cross-validation datasets and convert all the files from json to parquet","metadata":{}},{"cell_type":"code","source":"# Use code, provided by organizers to produce the first cross-validation daraset.\ntrain = Path('/kaggle/input/otto-recommender-system/train.jsonl')\noutput = Path('../')\nmain(train, output, 7, 367)","metadata":{"execution":{"iopub.status.busy":"2022-12-16T13:27:00.333405Z","iopub.execute_input":"2022-12-16T13:27:00.333795Z","iopub.status.idle":"2022-12-16T14:16:27.932489Z","shell.execute_reply.started":"2022-12-16T13:27:00.333762Z","shell.execute_reply":"2022-12-16T14:16:27.930661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert cv1 labels into tabular data.\ncv_labels_path = Path('../test_labels.jsonl')\ndf_cv_labels = save_labels(cv_labels_path)\nexport_file_cv_labels = 'cv_labels.parquet'\ndf_cv_labels.to_parquet(export_file_cv_labels)\n\ndel df_cv_labels\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Converting cv1 and test trancated sessions and full sessions into tabular data and saving them as parquet files.\nload_save(Path('../train_sessions.jsonl'), 'cv_train.parquet')\nload_save(Path('../test_sessions.jsonl'), 'cv_inputs.parquet')\nload_save(Path('/kaggle/input/otto-recommender-system/train.jsonl'), 'train_full.parquet')\nload_save(Path('/kaggle/input/otto-recommender-system/test.jsonl'), 'test.parquet')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Repeat the same process with different random seed to get an alternative cross-validation set (second cross-validation set).\nmain(train, output, 7, 203)\nload_save(Path('../test_sessions.jsonl'), 'cv_inputs2.parquet')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert alternative cross-validation set labels (cv2 labels) into tabular data and save them as parquet files.\ncv_labels_path = Path('../test_labels.jsonl')\ndf_cv_labels = save_labels(cv_labels_path)\nexport_file_cv_labels = 'cv_labels2.parquet'\ndf_cv_labels.to_parquet(export_file_cv_labels)\n\ndel df_cv_labels\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]}]}